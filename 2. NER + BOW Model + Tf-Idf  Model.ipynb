{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Named Entity Recognization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "paragraph = \"\"\"The Taj Mahal was built by Emperor Shah Jahan\"\"\"\n",
    "               \n",
    "               \n",
    "# POS Tagging\n",
    "words = nltk.word_tokenize(paragraph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_words = nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named entity recognition\n",
    "namedEnt = nltk.ne_chunk(tagged_words)\n",
    "namedEnt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "orga = \"\"\"\n",
    "ORGANIZATION\tGeorgia-Pacific Corp., WHO\n",
    "PERSON\tEddy Bonte, President Obama\n",
    "LOCATION\tMurray River, Mount Everest\n",
    "DATE\tJune, 2008-06-29\n",
    "TIME\ttwo fifty a m, 1:30 p.m.\n",
    "MONEY\t175 million Canadian Dollars, GBP 10.40\n",
    "PERCENT\ttwenty pct, 18.75 %\n",
    "FACILITY\tWashington Monument, Stonehenge\n",
    "GPE\tSouth East Asia, \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(orga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_words_new = nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameentity = nltk.ne_chunk(tagged_words_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameentity.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text Visuatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "paragraph = \"\"\"Thank you all so very much. Thank you to the Academy. \n",
    "               Thank you to all of you in this room. I have to congratulate \n",
    "               the other incredible nominees this year. The Revenant was \n",
    "               the product of the tireless efforts of an unbelievable cast\n",
    "               and crew. First off, to my brother in this endeavor, Mr. Tom \n",
    "               Hardy. Tom, your talent on screen can only be surpassed by \n",
    "               your friendship off screen … thank you for creating a t\n",
    "               ranscendent cinematic experience. Thank you to everybody at \n",
    "               Fox and New Regency … my entire team. I have to thank \n",
    "               everyone from the very onset of my career … To my parents; \n",
    "               none of this would be possible without you. And to my \n",
    "               friends, I love you dearly; you know who you are. And lastly,\n",
    "               I just want to say this: Making The Revenant was about\n",
    "               man's relationship to the natural world. A world that we\n",
    "               collectively felt in 2015 as the hottest year in recorded\n",
    "               history. Our production needed to move to the southern\n",
    "               tip of this planet just to be able to find snow. Climate\n",
    "               change is real, it is happening right now. It is the most\n",
    "               urgent threat facing our entire species, and we need to work\n",
    "               collectively together and stop procrastinating. We need to\n",
    "               support leaders around the world who do not speak for the \n",
    "               big polluters, but who speak for all of humanity, for the\n",
    "               indigenous people of the world, for the billions and \n",
    "               billions of underprivileged people out there who would be\n",
    "               most affected by this. For our children’s children, and \n",
    "               for those people out there whose voices have been drowned\n",
    "               out by the politics of greed. I thank you all for this \n",
    "               amazing award tonight. Let us not take this planet for \n",
    "               granted. I do not take tonight for granted. Thank you so very much.\"\"\"\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " we need BOW model becaues any ml or dl algo only understands numbers or float so the BOWM is a necessaty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data[i] = data[i].lower()\n",
    "    #lowercasing the dataset\n",
    "    data[i] = re.sub(r\"\\W\",' ',data[i])\n",
    "    # puncuation chars\n",
    "    data[i] = re.sub(r\"\\s+\",' ',data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank you all so very much ',\n",
       " 'thank you to the academy ',\n",
       " 'thank you to all of you in this room ',\n",
       " 'i have to congratulate the other incredible nominees this year ',\n",
       " 'the revenant was the product of the tireless efforts of an unbelievable cast and crew ',\n",
       " 'first off to my brother in this endeavor mr tom hardy ',\n",
       " 'tom your talent on screen can only be surpassed by your friendship off screen thank you for creating a t ranscendent cinematic experience ',\n",
       " 'thank you to everybody at fox and new regency my entire team ',\n",
       " 'i have to thank everyone from the very onset of my career to my parents none of this would be possible without you ',\n",
       " 'and to my friends i love you dearly you know who you are ',\n",
       " 'and lastly i just want to say this making the revenant was about man s relationship to the natural world ',\n",
       " 'a world that we collectively felt in 2015 as the hottest year in recorded history ',\n",
       " 'our production needed to move to the southern tip of this planet just to be able to find snow ',\n",
       " 'climate change is real it is happening right now ',\n",
       " 'it is the most urgent threat facing our entire species and we need to work collectively together and stop procrastinating ',\n",
       " 'we need to support leaders around the world who do not speak for the big polluters but who speak for all of humanity for the indigenous people of the world for the billions and billions of underprivileged people out there who would be most affected by this ',\n",
       " 'for our children s children and for those people out there whose voices have been drowned out by the politics of greed ',\n",
       " 'i thank you all for this amazing award tonight ',\n",
       " 'let us not take this planet for granted ',\n",
       " 'i do not take tonight for granted ',\n",
       " 'thank you so very much ']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating The histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#per line tokenizing\n",
    "word2count = {}\n",
    "for datas in data:\n",
    "    words = nltk.word_tokenize(datas)\n",
    "    for word in words:\n",
    "        if word not in word2count.keys():\n",
    "            word2count[word] = 1\n",
    "            #jodi na thake tale atleast \n",
    "            #it has apppeared once\n",
    "            \n",
    "        else:\n",
    "            word2count[word] += 1\n",
    "            #jodi thake tale count barachi            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thank': 8,\n",
       " 'you': 12,\n",
       " 'all': 4,\n",
       " 'so': 2,\n",
       " 'very': 3,\n",
       " 'much': 2,\n",
       " 'to': 16,\n",
       " 'the': 17,\n",
       " 'academy': 1,\n",
       " 'of': 10,\n",
       " 'in': 4,\n",
       " 'this': 9,\n",
       " 'room': 1,\n",
       " 'i': 6,\n",
       " 'have': 3,\n",
       " 'congratulate': 1,\n",
       " 'other': 1,\n",
       " 'incredible': 1,\n",
       " 'nominees': 1,\n",
       " 'year': 2,\n",
       " 'revenant': 2,\n",
       " 'was': 2,\n",
       " 'product': 1,\n",
       " 'tireless': 1,\n",
       " 'efforts': 1,\n",
       " 'an': 1,\n",
       " 'unbelievable': 1,\n",
       " 'cast': 1,\n",
       " 'and': 8,\n",
       " 'crew': 1,\n",
       " 'first': 1,\n",
       " 'off': 2,\n",
       " 'my': 5,\n",
       " 'brother': 1,\n",
       " 'endeavor': 1,\n",
       " 'mr': 1,\n",
       " 'tom': 2,\n",
       " 'hardy': 1,\n",
       " 'your': 2,\n",
       " 'talent': 1,\n",
       " 'on': 1,\n",
       " 'screen': 2,\n",
       " 'can': 1,\n",
       " 'only': 1,\n",
       " 'be': 4,\n",
       " 'surpassed': 1,\n",
       " 'by': 3,\n",
       " 'friendship': 1,\n",
       " 'for': 10,\n",
       " 'creating': 1,\n",
       " 'a': 2,\n",
       " 't': 1,\n",
       " 'ranscendent': 1,\n",
       " 'cinematic': 1,\n",
       " 'experience': 1,\n",
       " 'everybody': 1,\n",
       " 'at': 1,\n",
       " 'fox': 1,\n",
       " 'new': 1,\n",
       " 'regency': 1,\n",
       " 'entire': 2,\n",
       " 'team': 1,\n",
       " 'everyone': 1,\n",
       " 'from': 1,\n",
       " 'onset': 1,\n",
       " 'career': 1,\n",
       " 'parents': 1,\n",
       " 'none': 1,\n",
       " 'would': 2,\n",
       " 'possible': 1,\n",
       " 'without': 1,\n",
       " 'friends': 1,\n",
       " 'love': 1,\n",
       " 'dearly': 1,\n",
       " 'know': 1,\n",
       " 'who': 4,\n",
       " 'are': 1,\n",
       " 'lastly': 1,\n",
       " 'just': 2,\n",
       " 'want': 1,\n",
       " 'say': 1,\n",
       " 'making': 1,\n",
       " 'about': 1,\n",
       " 'man': 1,\n",
       " 's': 2,\n",
       " 'relationship': 1,\n",
       " 'natural': 1,\n",
       " 'world': 4,\n",
       " 'that': 1,\n",
       " 'we': 3,\n",
       " 'collectively': 2,\n",
       " 'felt': 1,\n",
       " '2015': 1,\n",
       " 'as': 1,\n",
       " 'hottest': 1,\n",
       " 'recorded': 1,\n",
       " 'history': 1,\n",
       " 'our': 3,\n",
       " 'production': 1,\n",
       " 'needed': 1,\n",
       " 'move': 1,\n",
       " 'southern': 1,\n",
       " 'tip': 1,\n",
       " 'planet': 2,\n",
       " 'able': 1,\n",
       " 'find': 1,\n",
       " 'snow': 1,\n",
       " 'climate': 1,\n",
       " 'change': 1,\n",
       " 'is': 3,\n",
       " 'real': 1,\n",
       " 'it': 2,\n",
       " 'happening': 1,\n",
       " 'right': 1,\n",
       " 'now': 1,\n",
       " 'most': 2,\n",
       " 'urgent': 1,\n",
       " 'threat': 1,\n",
       " 'facing': 1,\n",
       " 'species': 1,\n",
       " 'need': 2,\n",
       " 'work': 1,\n",
       " 'together': 1,\n",
       " 'stop': 1,\n",
       " 'procrastinating': 1,\n",
       " 'support': 1,\n",
       " 'leaders': 1,\n",
       " 'around': 1,\n",
       " 'do': 2,\n",
       " 'not': 3,\n",
       " 'speak': 2,\n",
       " 'big': 1,\n",
       " 'polluters': 1,\n",
       " 'but': 1,\n",
       " 'humanity': 1,\n",
       " 'indigenous': 1,\n",
       " 'people': 3,\n",
       " 'billions': 2,\n",
       " 'underprivileged': 1,\n",
       " 'out': 3,\n",
       " 'there': 2,\n",
       " 'affected': 1,\n",
       " 'children': 2,\n",
       " 'those': 1,\n",
       " 'whose': 1,\n",
       " 'voices': 1,\n",
       " 'been': 1,\n",
       " 'drowned': 1,\n",
       " 'politics': 1,\n",
       " 'greed': 1,\n",
       " 'amazing': 1,\n",
       " 'award': 1,\n",
       " 'tonight': 2,\n",
       " 'let': 1,\n",
       " 'us': 1,\n",
       " 'take': 2,\n",
       " 'granted': 2}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "# used to find the N most frequent words in the dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "freq_words = heapq.nlargest(100, \n",
    "                          word2count, \n",
    "                          key=word2count.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'to',\n",
       " 'you',\n",
       " 'of',\n",
       " 'for',\n",
       " 'this',\n",
       " 'thank',\n",
       " 'and',\n",
       " 'i',\n",
       " 'my',\n",
       " 'all',\n",
       " 'in',\n",
       " 'be',\n",
       " 'who',\n",
       " 'world',\n",
       " 'very',\n",
       " 'have',\n",
       " 'by',\n",
       " 'we',\n",
       " 'our',\n",
       " 'is',\n",
       " 'not',\n",
       " 'people',\n",
       " 'out',\n",
       " 'so',\n",
       " 'much',\n",
       " 'year',\n",
       " 'revenant',\n",
       " 'was',\n",
       " 'off',\n",
       " 'tom',\n",
       " 'your',\n",
       " 'screen',\n",
       " 'a',\n",
       " 'entire',\n",
       " 'would',\n",
       " 'just',\n",
       " 's',\n",
       " 'collectively',\n",
       " 'planet',\n",
       " 'it',\n",
       " 'most',\n",
       " 'need',\n",
       " 'do',\n",
       " 'speak',\n",
       " 'billions',\n",
       " 'there',\n",
       " 'children',\n",
       " 'tonight',\n",
       " 'take',\n",
       " 'granted',\n",
       " 'academy',\n",
       " 'room',\n",
       " 'congratulate',\n",
       " 'other',\n",
       " 'incredible',\n",
       " 'nominees',\n",
       " 'product',\n",
       " 'tireless',\n",
       " 'efforts',\n",
       " 'an',\n",
       " 'unbelievable',\n",
       " 'cast',\n",
       " 'crew',\n",
       " 'first',\n",
       " 'brother',\n",
       " 'endeavor',\n",
       " 'mr',\n",
       " 'hardy',\n",
       " 'talent',\n",
       " 'on',\n",
       " 'can',\n",
       " 'only',\n",
       " 'surpassed',\n",
       " 'friendship',\n",
       " 'creating',\n",
       " 't',\n",
       " 'ranscendent',\n",
       " 'cinematic',\n",
       " 'experience',\n",
       " 'everybody',\n",
       " 'at',\n",
       " 'fox',\n",
       " 'new',\n",
       " 'regency',\n",
       " 'team',\n",
       " 'everyone',\n",
       " 'from',\n",
       " 'onset',\n",
       " 'career',\n",
       " 'parents',\n",
       " 'none',\n",
       " 'possible',\n",
       " 'without',\n",
       " 'friends',\n",
       " 'love',\n",
       " 'dearly',\n",
       " 'know',\n",
       " 'are',\n",
       " 'lastly']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the BOG model\n",
    "\n",
    "X = [] # it will contain the BOW model\n",
    "\n",
    "# in a BOw each words is a vector 1 - apppears & 0- not appears\n",
    "for datas in data:\n",
    "    vector = []\n",
    "    for word in freq_words:\n",
    "        if word in nltk.word_tokenize(datas):\n",
    "            vector.append(1)\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    X.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 100)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.asarray(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text modelling using TF - IDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import heapq\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"Thank you all so very much. Thank you to the Academy. \n",
    "               Thank you to all of you in this room. I have to congratulate \n",
    "               the other incredible nominees this year. The Revenant was \n",
    "               the product of the tireless efforts of an unbelievable cast\n",
    "               and crew. First off, to my brother in this endeavor, Mr. Tom \n",
    "               Hardy. Tom, your talent on screen can only be surpassed by \n",
    "               your friendship off screen … thank you for creating a t\n",
    "               ranscendent cinematic experience. Thank you to everybody at \n",
    "               Fox and New Regency … my entire team. I have to thank \n",
    "               everyone from the very onset of my career … To my parents; \n",
    "               none of this would be possible without you. And to my \n",
    "               friends, I love you dearly; you know who you are. And lastly,\n",
    "               I just want to say this: Making The Revenant was about\n",
    "               man's relationship to the natural world. A world that we\n",
    "               collectively felt in 2015 as the hottest year in recorded\n",
    "               history. Our production needed to move to the southern\n",
    "               tip of this planet just to be able to find snow. Climate\n",
    "               change is real, it is happening right now. It is the most\n",
    "               urgent threat facing our entire species, and we need to work\n",
    "               collectively together and stop procrastinating. We need to\n",
    "               support leaders around the world who do not speak for the \n",
    "               big polluters, but who speak for all of humanity, for the\n",
    "               indigenous people of the world, for the billions and \n",
    "               billions of underprivileged people out there who would be\n",
    "               most affected by this. For our children’s children, and \n",
    "               for those people out there whose voices have been drowned\n",
    "               out by the politics of greed. I thank you all for this \n",
    "               amazing award tonight. Let us not take this planet for \n",
    "               granted. I do not take tonight for granted. Thank you so very much.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences\n",
    "dataset = nltk.sent_tokenize(paragraph)\n",
    "for i in range(len(dataset)):\n",
    "    dataset[i] = dataset[i].lower()\n",
    "    dataset[i] = re.sub(r'\\W',' ',dataset[i])\n",
    "    dataset[i] = re.sub(r'\\s+',' ',dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#per line tokenizing\n",
    "# Creating word histogram\n",
    "word2count = {}\n",
    "for data in dataset:\n",
    "    words = nltk.word_tokenize(data)\n",
    "    for word in words:\n",
    "        if word not in word2count.keys():\n",
    "            word2count[word] = 1\n",
    "            #jodi na thake tale atleast \n",
    "            #it has apppeared once\n",
    "        else:\n",
    "            word2count[word] += 1\n",
    "            #jodi thake tale count barachi  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting best 100 features\n",
    "freq_words = heapq.nlargest(100,\n",
    "                            word2count,\n",
    "                            key=word2count.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the BOG model\n",
    "\n",
    "X = [] # it will contain the BOW model\n",
    "\n",
    "# in a BOw each words is a vector 1 - apppears & 0- not appears\n",
    "for datas in data:\n",
    "    vector = []\n",
    "    for word in freq_words:\n",
    "        if word in nltk.word_tokenize(datas):\n",
    "            vector.append(1)\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    X.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDF Dictionary\n",
    "\n",
    "word_idfs = {}\n",
    "for word in freq_words:\n",
    "    doc_count = 0\n",
    "    for data in dataset:\n",
    "        if word in nltk.word_tokenize(data):\n",
    "            doc_count += 1\n",
    "    word_idfs[word] = np.log(len(dataset)/(1+doc_count))\n",
    "    # +1 is kindof like a bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF Matrix\n",
    "tf_matrix = {}\n",
    "for word in freq_words:\n",
    "    doc_tf = []\n",
    "    for data in dataset:\n",
    "        frequency = 0\n",
    "        for w in nltk.word_tokenize(data):\n",
    "            if word == w:\n",
    "                frequency += 1\n",
    "        tf_word = frequency/len(nltk.word_tokenize(data))\n",
    "        doc_tf.append(tf_word)\n",
    "    tf_matrix[word] = doc_tf   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF - IDF Calculation\n",
    "tfidf_matrix  = []\n",
    "for word in tf_matrix.keys():\n",
    "    tfidf = []\n",
    "    for value in tf_matrix[word]:\n",
    "        score = value * word_idfs[word]\n",
    "        tfidf.append(score)\n",
    "    tfidf_matrix.append(tfidf)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.12932543, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.11192316, 0.06217953, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.12365622, 0.14838747, 0.16487497, ..., 0.        , 0.        ,\n",
       "        0.14838747],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.asarray(tfidf_matrix).astype('float64')\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 21)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 100)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.T\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
